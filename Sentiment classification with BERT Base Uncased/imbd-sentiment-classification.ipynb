{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/bert-base-uncased/vocab.txt\n/kaggle/input/bert-base-uncased/pytorch_model.bin\n/kaggle/input/bert-base-uncased/config.json\n/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\n\nDEVICE = \"cuda\"\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\n\nBERT_PATH = \"../input/bert-base-uncased\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\nBERT_PATH,\n    do_lower_case = True\n)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDataset:\n    def __init__(self,review,target):\n        self.review = review\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.review)\n    def _getitem__(self,item):\n        review = str(self.review)\n        review = \"\".join(review.split())\n        \n        #encodes two strings at a time\n        inputs = self.tokenizer.encode_plus(\n        review,\n        None,\n        add_special_tokens =True,#cls tokens\n        max_length = self.max_len\n        )  \n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        padding_length = self.max_len = len(ids)\n        ids = ids + ([0] + padding_length)\n        mask  = mask +([0] + padding_length)\n        token_type_ids = token_type_ids +([0] + padding_length)\n\n        return {\n            'ids' :  torch.Tensor(ids,dtype=torch.long),\n            'mask' : torch.Tensor(mask,dtype= torch.long),\n            'token_type_ids': torch.Tensor(token_type_ids, dtype=torch.long),\n            'target' : torch.Tensor(self.target[item],dtype=torch.float)\n        }\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm \nimport torch\n\n\ndef loss_fn():\n    return nn.BCEWithLogitsLoss()(outputs,targets)\n\ndef train_fn(data_loader,model,optimizer,device, scheduler):\n    model.train()\n    for bi,d in tqdm(enumerate(data_loader), total = len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids= d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n        \n        ids = ids.to(device,dtype=torch.long)\n        token_type_ids = token_type_ids.to(device,dtype=torch.long)\n        mask = mask.to(device,dtype=torch.long)\n        targets = targets.to(device,dtype=torch.float)\n        \n        optimizers.zero_grad()\n        outputs = model(\n            ids = ids, \n            mask = mask,\n            token_type_ids  =token_type_ids\n        )\n        \n        loss = loss_fn(outputs,targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n    def eval_fn(data_loader,model,device):\n        fin_train = []\n        fin_outputs = []\n        model.eval()\n        with torch.no_grad():\n            for bi,d in tqdm(enumerate(data_loader), total = len(data_loader)):\n                ids = d[\"ids\"]\n                token_type_ids= d[\"token_type_ids\"]\n                mask = d[\"mask\"]\n                targets = d[\"targets\"]\n\n                ids = ids.to(device,dtype=torch.long)\n                token_type_ids = token_type_ids.to(device,dtype=torch.long)\n                mask = mask.to(device,dtype=torch.long)\n                targets = targets.to(device,dtype=torch.float)\n\n\n                outputs = model(\n                    ids = ids, \n                    mask = mask,\n                    token_type_ids  = token_type_ids\n                    \n                )\n                \n                fin_targets.extend(targets.cpu().detach().numpy().tolist())\n                fin_outputs.extend(torch.sigmoid(targets).cpu().detach().numpy().tolist())\n           \n    return fin_targets,fin_outputs\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import model_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n\n\n\n\ndef run():\n    dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n    dfx.sentiment = dfx.sentiment.apply(\n    lambda x: 1 if x==\"positive\" else 0\n    )\n    \n    df_train,df_valid = model_selection.train_test_split(\n         dfx,\n        test_size = 0.1,\n        random_state = 42,\n        stratify  =dfx.sentiment.values\n    )\n    \n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n    \n    train_dataset = BERTDataset(\n        review=df_train.review.values, target=df_train.sentiment.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4\n    )\n\n    valid_dataset = BERTDataset(\n        review=df_valid.review.values, target=df_valid.sentiment.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1\n    )\n\n    device = torch.device(DEVICE)\n    model = BERTBaseUncased()\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n    )\n\n    best_accuracy = 0\n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n        outputs, targets = eval_fn(valid_data_loader, model, device)\n        outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f\"Accuracy Score = {accuracy}\")\n        if accuracy > best_accuracy:\n            torch.save(model.state_dict(), MODEL_PATH)\n            best_accuracy = accuracy\n\n\nif __name__ == \"__main__\":\n    run()\n    \n    ","execution_count":25,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"train_fn() takes 4 positional arguments but 5 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-d081677f61be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-d081677f61be>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: train_fn() takes 4 positional arguments but 5 were given"]}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sentiment =df.sentiment.apply(\nlambda x:1 if x==\"positive\" else 0\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"kfold\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df.sentiment.values\nkf = model_selection.StratifiedKFold(n_splits=5)\nfor f,(t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold']=f\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nclass IMDBDataset:\n    def __init__(self,reviews,targets):\n        self.reviews = reviews\n        self.target = targets\n    def __len__(self):\n        return len(self.reviews)\n    def __getitem__(self,item):\n        review = self.reviews[item,:]\n        target = self.target[item]\n        \n        return{\n            \"review\" : torch.tensor(review,dtype = torch.long),\n            \"target\" : torch.tensor(target,dtype= torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nclass LSTM(nn.Module):\n    def __init__(self,eembedding_matrix):\n        super(LSTM,self).__init__()\n        num_words = embedding_matrix.shape[0]\n        embed_dim = embedding_matrix.shape[1]\n        self.embedding = nn.Embedding(num_embedding=num_words,embedding_dim = embed_dim)\n\n        self.embedding.weight=nn.Parameter(torch.tensor(\n        embedding_matrix,\n            dtype=torch.float32\n        )\n                                          )\n\n        self.embedding.weight.requires_grad=False\n\n        self.lstm = nn.LSTM(embed_dim,\n                           128,bidirectional=True,batch=True)\n\n        self.out = nn.Linear(512,1)\n    def forward(self,x):\n        x= self.embedding(x)\n        x,_= self.lstm(x)\n        avg_pool = torch.mean(x,1)\n        max_pool,_=torch.max(x,1)\n        \n        \n        out=torch.cat((avg_pool,max_pool),1)\n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}